{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "byjJq4buneNq"
   },
   "source": [
    "## PART 1 - Deep Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WRXspKTlkRhg"
   },
   "outputs": [],
   "source": [
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eaZqvaHkkct5"
   },
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MwsApAJukjmq"
   },
   "outputs": [],
   "source": [
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DijD6TQJve-O"
   },
   "source": [
    "**Step 4. Preprocessing: Subtract the mean value of training images from every pixel in\n",
    "every image in both train and test data to shift the total mean to 0. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eTVQMu3TvUE4"
   },
   "outputs": [],
   "source": [
    "train_image_list = []\n",
    "for image in train_images:\n",
    "    mean = np.mean(image)\n",
    "    train_image_list.append(np.subtract(image, np.full((28, 28), mean)))\n",
    "  \n",
    "test_image_list = []\n",
    "for image in test_images:\n",
    "  mean = np.mean(image)\n",
    "  test_image_list.append(np.subtract(image, np.full((28, 28), mean)))\n",
    "  \n",
    "train_images = np.stack(train_image_list)\n",
    "test_images = np.stack(test_image_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uIMQ5GeFc8bi"
   },
   "source": [
    "**Step 6. Training & Validation: Split the training data into 30% validation and 70% training.**\n",
    "\n",
    " The following cell which splits validation and training data is used only for this step. Since ImageDataGenerator can split validation and training data, it is used for splitting from Step 7 on.\n",
    "\n",
    "Uncomment it for Step 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_kmIl4xrdBsP"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=5, random_state=0, test_size=0.3)\n",
    "\n",
    "train_index, valid_index = next(sss.split(train_images, train_labels))\n",
    "\n",
    "valid_images, valid_labels = train_images[valid_index], train_labels[valid_index]\n",
    "train_images, train_labels = train_images[train_index], train_labels[train_index]\n",
    "\n",
    "print(train_images.shape, valid_images.shape, test_images.shape)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SjcISfJw_Hca"
   },
   "source": [
    "**Step 5. Convolutional Layer: For adding a convolutional layer you need to reshape your\n",
    "data to 4 dimensions (60000, 28,28) -> (60000, 28, 28, 1).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xKfCVR9CkuJm"
   },
   "outputs": [],
   "source": [
    "train_images = np.expand_dims(train_images, axis=3)  \n",
    "print(\"New shape of train_images is \", train_images.shape)\n",
    "test_images = np.expand_dims(test_images, axis=3)\n",
    "print(\"New shape of test_images is \", test_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ObCDkKZxk4PC"
   },
   "outputs": [],
   "source": [
    "train_images = train_images / 255.0\n",
    "\n",
    "test_images = test_images / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5dCeKSJk2xf8"
   },
   "source": [
    "**Step 3. Weight Initializer: Compare using random_normal and random_uniform as your kernel_initializer parameters in your Dense layer.** Here are the models that I have tried and their accuracies:\n",
    "\n",
    "1.   model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(10, activation=tf.nn.softmax, kernel_initializer=\"random_normal\")\n",
    "])   \n",
    "Test accuracy: 0.8622\n",
    "2.   model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(10, activation=tf.nn.softmax, kernel_initializer=\"random_uniform\")\n",
    "])    \n",
    "Test accuracy: 0.8708\n",
    "3.  model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(128, activation=tf.nn.relu, kernel_initializer=\"random_normal\"),\n",
    "    keras.layers.Dense(10, activation=tf.nn.softmax, kernel_initializer=\"random_normal\")\n",
    "])       \n",
    "Test accuracy: 0.8749\n",
    "4.  model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(128, activation=tf.nn.relu, kernel_initializer=\"random_uniform\"),\n",
    "    keras.layers.Dense(10, activation=tf.nn.softmax, kernel_initializer=\"random_uniform\")\n",
    "])       \n",
    "Test accuracy: 0.8718\n",
    "\n",
    "\n",
    "Model 3 is used since it has the highest accuracy among others.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**Step 5. Convolutional Layer: As the first layer add a new 3x3 convolutional layer with 128 filters.**\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "**Step 9. Regularization: Add DropOut layer(s) for regularization where it makes sense.**\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "**Step 10.Batch Normalization: Add BatchNormalization layer(s) where it makes sense.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GgVlmw_vk4n4"
   },
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Conv2D(filters=128, kernel_size=3, input_shape=(28, 28, 1)),\n",
    "    keras.layers.Dropout(0.25),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(128, kernel_initializer=\"random_normal\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation('relu'),\n",
    "    keras.layers.Dropout(0.25),\n",
    "    keras.layers.Dense(10, kernel_initializer=\"random_normal\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation('softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3ac2eyrFeYJ9"
   },
   "source": [
    "**Step 11. Loss Function: Change sparse_categorical_crossentropy loss to categorical_crossentropy loss first. Experiment. Then change it to cosine_proximity loss. Experiment. Explain which loss function fits the data best and why?**\n",
    "\n",
    "For TPU but without SGD optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HndXPv_Mv098"
   },
   "outputs": [],
   "source": [
    "tpu_model = tf.contrib.tpu.keras_to_tpu_model(\n",
    "    model,\n",
    "    strategy=tf.contrib.tpu.TPUDistributionStrategy(\n",
    "        tf.contrib.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
    "    )\n",
    ")\n",
    "tpu_model.compile(optimizer=tf.train.AdamOptimizer(),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              #loss='categorical_crossentropy',\n",
    "              #loss='cosine_proximity',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h-ywL0UfvqKo"
   },
   "source": [
    "**Step 8. Different Optimizer: Change the optimizer to use Stochastic Gradient Descent\n",
    "(SGD) with the following parameters: (lr=0.1, momentum=0.7, decay=0.01, nesterov=True) Explain each of these parameters. **   \n",
    "Arguments:\n",
    "* lr: float >= 0. Learning rate. Gradient descent algorithms multiply the gradient by a scalar known as the learning rate (also sometimes called step size) to determine the next point.\n",
    "* momentum: float >= 0. Parameter updates momentum. SGD with momentum is method which helps accelerate gradients vectors in the right directions, thus leading to faster converging.\n",
    "* decay: float >= 0. Learning rate decay over each update. Weight update rule that causes the weights to exponentially decay to zero\n",
    "* nesterov: boolean. Whether to apply Nesterov momentum. Nesterov momentum has slightly less overshooting compare to standard momentum since it takes the \"gamble->correction\" approach.\n",
    "\n",
    "Use CPU this step on.\n",
    "\n",
    "---\n",
    "\n",
    "**Step 11. Loss Function: Change sparse_categorical_crossentropy loss to\n",
    "categorical_crossentropy loss first. Experiment. Then change it to\n",
    "cosine_proximity loss. Experiment. Explain which loss function fits the data best\n",
    "and why?**\n",
    "\n",
    "For CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1kWECS_5lyjc"
   },
   "outputs": [],
   "source": [
    "model.compile(keras.optimizers.SGD(lr=0.1, momentum=0.7, decay=0.01, nesterov=True),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              #loss='categorical_crossentropy',\n",
    "              #loss='cosine_proximity',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sMIo_8jVtcCE"
   },
   "source": [
    "**Step 6. Training & Validation: Add an EarlyStopping Keras callback with appropriate parameters to\n",
    "stop the training epochs if validation accuracy has not improved for 3 epochs (run\n",
    "your network training for 50 epochs if no early stop happens. Use the\n",
    "fit_generator function instead of fit).**\n",
    "\n",
    "The following cells which take validation and training data seperately are used only for this step. Since ImageDataGenerator can split validation and training data, it is used for splitting from Step 7 on.\n",
    "\n",
    "Uncomment them for Step 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LrFtVUq_lR6T"
   },
   "outputs": [],
   "source": [
    "callback = [keras.callbacks.EarlyStopping(monitor='loss', patience=3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0dEe2EdAzqbB"
   },
   "outputs": [],
   "source": [
    "def train_gen(batch_size):\n",
    "  while True:\n",
    "    offset = np.random.randint(0, train_images.shape[0] - batch_size)\n",
    "    yield train_images[offset:offset+batch_size], train_labels[offset:offset + batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "79LPTOmxlyr4"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# For CPU usage\n",
    "model.fit_generator(\n",
    "    train_gen(512),\n",
    "    epochs=50,\n",
    "    steps_per_epoch=100,\n",
    "    validation_data=(valid_images, valid_labels),\n",
    "    callbacks = [keras.callbacks.EarlyStopping(monitor='loss', patience=3)]\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4H93qqy8wb21"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# For TPU usage\n",
    "tpu_model.fit_generator(\n",
    "    train_gen(512),\n",
    "    epochs=50,\n",
    "    steps_per_epoch=100,\n",
    "    validation_data=(valid_images, valid_labels),\n",
    "    callbacks = [keras.callbacks.EarlyStopping(monitor='loss', patience=3)]\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VpWfmH-zf0MN"
   },
   "source": [
    "**Step 6. Training & Validation: Split the training data into 30% validation and 70% training.**\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "**Step 7. Data Augmentation: Augment the training data by adding only horizontal flips of\n",
    "the training images. Also experiment with augmenting data with only vertical flips.\n",
    "Explain which one gives the best results. Explain your reasoning. You may use\n",
    "keras.preprocessing.image.ImageDataGenerator class. After this step continue\n",
    "with augmenting the data with horizontal flips only.**\n",
    "\n",
    "\n",
    "Data augmentation is used for basically enlargening the dataset. For instance, as we flipped the images, we created \"new\" data with different orientations. Data augmentation is useful when the dataset variety is limited.\n",
    "\n",
    "For vertical flip, delete  ```horizontal_flip=True``` and add \n",
    "```vertical_flip=True```.\n",
    "\n",
    "\n",
    "Commented part (TPU) can be used for Step 7 - 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x0t6Ot2t1i6G"
   },
   "outputs": [],
   "source": [
    "datagen = keras.preprocessing.image.ImageDataGenerator(horizontal_flip=True, validation_split=0.3)\n",
    "datagen.fit(train_images)\n",
    "\n",
    "\"\"\"\n",
    "tpu_model.fit_generator(datagen.flow(train_images, train_labels, batch_size=512),\n",
    "                       epochs=50,\n",
    "                       steps_per_epoch=100,\n",
    "                       callbacks = callback)\n",
    "\"\"\"\n",
    "\n",
    "model.fit_generator(datagen.flow(train_images, train_labels, batch_size=512),\n",
    "                       epochs=50,\n",
    "                       steps_per_epoch=100,\n",
    "                       callbacks = callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GmV2aAXFukJ3"
   },
   "source": [
    "**Step 6. Training & Validationâ€‹: Split the training data into 30% validation and 70% training.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4bUcHzf04q9A"
   },
   "outputs": [],
   "source": [
    "model.fit(train_images, train_labels, validation_split=0.3, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c01yFP0f0mlj"
   },
   "outputs": [],
   "source": [
    "test_loss, test_acc = tpu_model.evaluate(test_images, test_labels)\n",
    "\n",
    "print('Test accuracy for TPU:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mPkLB1NZlyw4"
   },
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "\n",
    "print('Test accuracy for CPU:', test_acc)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "PART 1 - Deep Neural Networks",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
